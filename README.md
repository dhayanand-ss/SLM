🧠 Small Language Model (SLM) — From Scratch

SLM is an educational project that builds a small-scale GPT-style transformer (≈50–60M parameters) capable of creative and coherent text generation.
It’s designed to be computationally accessible, while still following modern large language model (LLM) design principles.

🎯 Project Goal

Train a GPT-like transformer language model from scratch, optimized for:

Short story generation

Children’s vocabulary and comprehension levels

📚 Dataset

Dataset: TinyStories

A synthetic dataset of short, simple stories generated by GPT-3.5 and GPT-4, featuring child-friendly vocabulary and clear sentence structure — perfect for training smaller models.

✨ Features

✅ Full from-scratch implementation:

Custom tokenization, batching, model, and optimization
✅ Disk-efficient tokenization & data loading
✅ Modern GPT architecture:

Causal self-attention

MLP with GELU activation

Embedding layers with tied weights
✅ Training:

GPU/CPU support

Mixed-precision (AMP) for faster training
✅ Optimization:

AdamW optimizer

Cosine learning rate scheduler with warmup
✅ Progress tracking:

Real-time loss logging

Validation & checkpoint saving

Training/validation loss visualization
✅ Text generation:

.generate() method for sampling and inference

🧩 Requirements

Python 3.9+

PyTorch

NumPy, Pandas, Tqdm

Hugging Face Datasets & Tokenizers

tiktoken (for GPT-2-style tokenization)

Matplotlib

Install all dependencies:

pip install torch numpy pandas tqdm datasets tokenizers tiktoken matplotlib

⚙️ Usage
1. Import and Prepare Dataset

Automatically loads TinyStories from Hugging Face.

Tokenizes text and saves to disk as:

train.bin

validation.bin

2. Create Batches

Batching logic for both train/validation splits.

Prepares autoregressive (input → next token) pairs.

3. Define Model Architecture

Custom GPT config with:

Attention layers

Layer normalization

MLP blocks

Weight tying for embeddings

4. Train the Model

Uses cross-entropy loss and AdamW optimizer.

Supports mixed-precision training for better performance.

Includes cosine LR scheduler with warmup steps.

Logs and plots training/validation loss over time.

Automatically saves the best model (bestmodelparams.pt).

5. Evaluate & Generate

Load best checkpoint and test text generation with:

model.generate(prompt="Once upon a time,")

📊 Monitoring Training

At the end of training:

View and save loss plots.

Optionally resume from a checkpoint for continued training.

📁 File Structure
SLM.ipynb              # Main notebook (data, model, training, evaluation)
train.bin              # Tokenized training data
validation.bin         # Tokenized validation data
bestmodelparams.pt     # Checkpoint with lowest validation loss

⚙️ Configuration

Modify model hyperparameters directly in the notebook:

Number of layers / attention heads

Embedding size

Block size (context length)

Batch size / learning rate

💡 The default configuration (~50–60M parameters) is optimized for educational use.
For larger datasets or extended training, consider using distributed storage and more scalable batching.

🧠 Inspiration & Credits

Dataset: TinyStories

Architecture Reference: Inspired by Andrej Karpathy’s nanoGPT
 and GPT research papers

Project Goal: Serve as a clear, accessible educational tool for understanding modern transformer-based LLMs

📜 License

MIT License — see full license text in the notebook.

🌟 Summary

SLM is a hands-on, approachable way to:

Understand how GPT-style models are built and trained

Experiment with tokenization, architecture design, and optimization

Generate coherent short stories using a model small enough to train on a single GPU or even CPU (with patience!)
