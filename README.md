Small Language Model (SLM) From Scratch
This project builds a Small Language Model with ~50-60M parameters for creative and coherent text generation. It follows modern LLM design while being computationally accessible.

Overview
Goal: Train a GPT-style transformer language model from scratch, tuned for short stories and children’s vocabulary.

Dataset: TinyStories — synthetic short stories using simple vocabulary, suitable for children, generated by GPT-3.5 & GPT-4.

Features
Full scratch implementation (tokenization, batching, architecture, optimization).

TinyStories dataset loading and disk-efficient tokenization.

Custom GPT config: attention, MLP, embedding, tied weights.

Training on GPU/CPU with mixed-precision support.

Loss tracking and best model saving.

Plotting train/validation loss curves for progress monitoring.

Requirements
Python 3.9+

PyTorch

Numpy, Pandas, Tqdm

HuggingFace Datasets, Tokenizers

tiktoken (for GPT-2 tokenization)

Matplotlib (for visualizations)

Install:

bash
pip install torch numpy pandas tqdm datasets tokenizers tiktoken matplotlib
Usage
1. Import and Prepare Dataset

Loads TinyStories from HuggingFace.

Tokenizes and writes tokens to disk as train.bin and validation.bin.

2. Batch Creation

Batching code for train/valid splits.

Prepares input/output pairs for autoregressive training.

3. Model Architecture

Define GPT config and model class.

Implements causal self-attention, layer norm, MLP, and full transformer block.

4. Loss & Trainer

Cross-entropy loss on outputs.

AdamW optimizer with cosine learning rate scheduling and warmup.

Support for AMP/mixed-precision on capable GPUs.

5. Training Loop

Tracks losses, plots curves, saves best checkpoint.

6. Evaluation & Inference

Model generation method demo.

Optionally, load/checkpoint best weights for sample text generation.

Example Run
Run notebook cells in order.

At the end of training, plot loss curves.

To generate text, use the .generate() method on the model with a prompt.

File Structure
SLM.ipynb: Main notebook (all steps: data, model, training, eval)

train.bin / validation.bin: Binarized tokenized data

bestmodelparams.pt: Saved PyTorch state with lowest validation loss

Notes
You may edit config to change model size, layers, block size, etc.

Model optimized for educational/experimental use; not production scale.

For larger datasets or production, migrate binarization/batching to more scalable disc/file systems.

Credits
TinyStories Dataset

Model architecture adapted from Andrej Karpathy’s nanoGPT and GPT-style references.

Project built as an educational and research tool.

License
MIT License (see notebook for full license text).

