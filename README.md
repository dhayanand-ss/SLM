ğŸ§  Small Language Model (SLM) â€” From Scratch

SLM is an educational project that builds a small-scale GPT-style transformer (â‰ˆ50â€“60M parameters) capable of creative and coherent text generation.
Itâ€™s designed to be computationally accessible, while still following modern large language model (LLM) design principles.

ğŸ¯ Project Goal

Train a GPT-like transformer language model from scratch, optimized for:

Short story generation

Childrenâ€™s vocabulary and comprehension levels

ğŸ“š Dataset

Dataset: TinyStories

A synthetic dataset of short, simple stories generated by GPT-3.5 and GPT-4, featuring child-friendly vocabulary and clear sentence structure â€” perfect for training smaller models.

âœ¨ Features

âœ… Full from-scratch implementation:

Custom tokenization, batching, model, and optimization
âœ… Disk-efficient tokenization & data loading
âœ… Modern GPT architecture:

Causal self-attention

MLP with GELU activation

Embedding layers with tied weights
âœ… Training:

GPU/CPU support

Mixed-precision (AMP) for faster training
âœ… Optimization:

AdamW optimizer

Cosine learning rate scheduler with warmup
âœ… Progress tracking:

Real-time loss logging

Validation & checkpoint saving

Training/validation loss visualization
âœ… Text generation:

.generate() method for sampling and inference

ğŸ§© Requirements

Python 3.9+

PyTorch

NumPy, Pandas, Tqdm

Hugging Face Datasets & Tokenizers

tiktoken (for GPT-2-style tokenization)

Matplotlib

Install all dependencies:

pip install torch numpy pandas tqdm datasets tokenizers tiktoken matplotlib

âš™ï¸ Usage
1. Import and Prepare Dataset

Automatically loads TinyStories from Hugging Face.

Tokenizes text and saves to disk as:

train.bin

validation.bin

2. Create Batches

Batching logic for both train/validation splits.

Prepares autoregressive (input â†’ next token) pairs.

3. Define Model Architecture

Custom GPT config with:

Attention layers

Layer normalization

MLP blocks

Weight tying for embeddings

4. Train the Model

Uses cross-entropy loss and AdamW optimizer.

Supports mixed-precision training for better performance.

Includes cosine LR scheduler with warmup steps.

Logs and plots training/validation loss over time.

Automatically saves the best model (bestmodelparams.pt).

5. Evaluate & Generate

Load best checkpoint and test text generation with:

model.generate(prompt="Once upon a time,")

ğŸ“Š Monitoring Training

At the end of training:

View and save loss plots.

Optionally resume from a checkpoint for continued training.

ğŸ“ File Structure
SLM.ipynb              # Main notebook (data, model, training, evaluation)
train.bin              # Tokenized training data
validation.bin         # Tokenized validation data
bestmodelparams.pt     # Checkpoint with lowest validation loss

âš™ï¸ Configuration

Modify model hyperparameters directly in the notebook:

Number of layers / attention heads

Embedding size

Block size (context length)

Batch size / learning rate

ğŸ’¡ The default configuration (~50â€“60M parameters) is optimized for educational use.
For larger datasets or extended training, consider using distributed storage and more scalable batching.

ğŸ§  Inspiration & Credits

Dataset: TinyStories

Architecture Reference: Inspired by Andrej Karpathyâ€™s nanoGPT
 and GPT research papers

Project Goal: Serve as a clear, accessible educational tool for understanding modern transformer-based LLMs

ğŸ“œ License

MIT License â€” see full license text in the notebook.

ğŸŒŸ Summary

SLM is a hands-on, approachable way to:

Understand how GPT-style models are built and trained

Experiment with tokenization, architecture design, and optimization

Generate coherent short stories using a model small enough to train on a single GPU or even CPU (with patience!)
